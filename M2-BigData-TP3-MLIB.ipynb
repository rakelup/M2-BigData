{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification binaire avec MLIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ce notebook est conçu pour vous aider à démarrer avec Apache Spark MLlib.\n",
    "\n",
    "Nous allons étudier un problème de classification binaire:\n",
    "**Prédire si le revenu d'un individu est supérieur à 50 000 $** à partir d'une base de données démographiques? \n",
    "\n",
    "L'ensemble de données provient du référentiel [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Adult) et est fourni avec Databricks Runtime. \n",
    "\n",
    "Ce bloc-notes présente certaines des fonctionnalités disponibles dans MLlib: notamment des outils de prétraitement des données, des pipelines d'apprentissage automatique et plusieurs algorithmes d'apprentissage automatique.\n",
    "\n",
    "<b>Ce cahier comprend les étapes suivantes:</b> \n",
    "\n",
    "0. Charger le jeu de données\n",
    "0. Nettoayage des données\n",
    "0. Définir le modèle\n",
    "0. Construisez le pipeline\n",
    "0. Évaluer le modèle\n",
    "0. Réglage des hyperparamètres\n",
    "0. Faire des prédictions et évaluer les performances du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Étape 0. Chargez l'ensemble de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher les premières lignes des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs head --maxBytes=1024 databricks-datasets/adult/adult.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un schéma pour attribuer des noms de colonne et des types de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"`age` DOUBLE,\n",
    "`workclass` STRING,\n",
    "`fnlwgt` DOUBLE,\n",
    "`education` STRING,\n",
    "`education_num` DOUBLE,\n",
    "`marital_status` STRING,\n",
    "`occupation` STRING,\n",
    "`relationship` STRING,\n",
    "`race` STRING,\n",
    "`sex` STRING,\n",
    "`capital_gain` DOUBLE,\n",
    "`capital_loss` DOUBLE,\n",
    "`hours_per_week` DOUBLE,\n",
    "`native_country` STRING,\n",
    "`income` STRING\"\"\"\n",
    "\n",
    "dataset = spark.read.csv(\"/databricks-datasets/adult/adult.data\", schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divisez aléatoirement les données en ensembles d'apprentissage et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF, testDF = dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "print(trainDF.cache().count()) # Cache car accès aux données d'entraînement plusieurs fois\n",
    "print(testDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualiser les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trainDF.select(\"hours_per_week\").summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trainDF\n",
    "        .groupBy(\"education\")\n",
    "        .count()\n",
    "        .sort(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers, estimators, and pipelines\n",
    "\n",
    "-**Transformer:** prend un DataFrame comme entrée et renvoie un nouveau DataFrame. Les transformateurs n'apprennent aucun paramètre à partir des données et appliquent simplement des transformations basées sur des règles pour **préparer les données** pour l'apprentissage du modèle **ou générer des prédictions** à l'aide d'un modèle MLlib entraîné. Vous appelez un transformateur avec une méthode .transform ().\n",
    "\n",
    "-**Estimator:** **apprend** (ou «ajuste») les paramètres de votre DataFrame via une méthode .fit () et renvoie un modèle, qui est un transformateur.\n",
    "\n",
    "-**Pipeline:** combine plusieurs étapes dans un flux de travail unique qui peut être facilement exécuté. La création d'un modèle d'apprentissage automatique implique généralement la configuration de nombreuses étapes différentes et leur itération. Les pipelines vous aident à automatiser ce processus.\n",
    "\n",
    "[ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html#ml-pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Étape 1. Nettoayage des données\n",
    "On veut construire un modèle qui prédit le niveau de revenu à partir des caractéristiques incluses dans l'ensemble de données (niveau d'éducation, état matrimonial, profession, etc.). La première étape consiste à manipuler ou prétraiter les données afin qu'elles soient au format requis par MLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir les variables catégorielles en variables numériques\n",
    "\n",
    "Certains algorithmes d'apprentissage automatique, tels que la régression linéaire et logistique, nécessitent des fonctionnalités numériques. L'ensemble de données Adultes comprend des caractéristiques catégoriques telles que l'éducation, la profession et l'état matrimonial.\n",
    "\n",
    "Le bloc de code suivant illustre comment utiliser `StringIndexer` et` OneHotEncoder` pour convertir des variables catégorielles en un ensemble de variables numériques qui ne prennent que les valeurs 0 et 1.\n",
    "\n",
    "- `StringIndexer` convertit une colonne de valeurs strings en une colonne d'index d'étiquette. Par exemple, il peut convertir les valeurs «rouge», «bleu» et «vert» en 0, 1 et 2.\n",
    "- `OneHotEncoder` mappe une colonne d'indices de catégorie à une colonne de vecteurs binaires, avec au plus un\" 1 \"dans chaque ligne qui indique l'index de catégorie pour cette ligne.\n",
    "\n",
    "L'encodage One-hot dans Spark est un processus en deux étapes. Vous utilisez d'abord le StringIndexer, suivi de OneHotEncoder. Le bloc de code suivant définit le StringIndexer et OneHotEncoder mais ne l'applique pas encore à des données.\n",
    "\n",
    "[StringIndexer](http://spark.apache.org/docs/latest/ml-features.html#stringindexer)   \n",
    "[OneHotEncoder](https://spark.apache.org/docs/latest/ml-features.html#onehotencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "categoricalCols = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\"]\n",
    "\n",
    "# The following two lines are estimators. They return functions that we will later apply to transform the dataset.\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=[x + \"Index\" for x in categoricalCols]) \n",
    "encoder = OneHotEncoder(inputCols=stringIndexer.getOutputCols(), outputCols=[x + \"OHE\" for x in categoricalCols]) \n",
    "\n",
    "# The label column (\"income\") is also a string value - it has two possible values, \"<=50K\" and \">50K\". \n",
    "# Convert it to a numeric value using StringIndexer.\n",
    "labelToIndex = StringIndexer(inputCol=\"income\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer un pipeline combinant toutes nos étapes de nettoayage et apprentisage. Mais prenons une minute pour examiner de plus près le fonctionnement des estimateurs et des transformateurs en appliquant l'estimateur `stringIndexer` que nous avons créé dans le bloc de code précédent.\n",
    "\n",
    "Vous pouvez appeler la méthode `.fit ()` pour renvoyer un `StringIndexerModel`, que vous pouvez ensuite utiliser pour transformer l'ensemble de données.\n",
    "\n",
    "La méthode `.transform ()` de `StringIndexerModel` renvoie un nouveau DataFrame avec les nouvelles colonnes ajoutées. Faites défiler vers la droite pour voir les nouvelles colonnes si nécessaire.\n",
    "\n",
    "Pour plus d'informations: [StringIndexerModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexerModel = stringIndexer.fit(trainDF)\n",
    "display(stringIndexerModel.transform(trainDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine toutes les colonnes d'entités en un seul vecteur d'entités\n",
    "\n",
    "La plupart des algorithmes MLlib nécessitent une seule colonne de fonctionnalités en entrée. Chaque ligne de cette colonne contient un vecteur de points de données correspondant à l'ensemble des caractéristiques utilisées pour la prédiction.\n",
    "\n",
    "MLlib fournit le transformateur `VectorAssembler` pour créer une seule colonne vectorielle à partir d'une liste de colonnes.\n",
    "\n",
    "Le bloc de code suivant montre comment utiliser VectorAssembler.\n",
    "\n",
    "Pour plus d'informations: [VectorAssembler] (https://spark.apache.org/docs/latest/ml-features.html#vectorassembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5f1fcfa00688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This includes both the numeric columns and the one-hot encoded binary vector columns in our dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnumericCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fnlwgt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"education_num\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"capital_gain\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"capital_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hours_per_week\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0massemblerInputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"OHE\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategoricalCols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumericCols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# This includes both the numeric columns and the one-hot encoded binary vector columns in our dataset.\n",
    "numericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "assemblerInputs = [c + \"OHE\" for c in categoricalCols] + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 2. Définir le modèle d'apprentisage\n",
    "\n",
    "On utilise un modèle de [régression logistique] (https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 3. Construisez le pipeline\n",
    "\n",
    "Un «Pipeline» est une liste ordonnée de transformateurs et d'estimateurs. Vous pouvez définir un pipeline pour automatiser et garantir la répétabilité des transformations à appliquer à un ensemble de données. Dans cette étape, nous définissons le pipeline, puis nous l'appliquons à l'ensemble de données de test.\n",
    "\n",
    "Semblable à ce que nous avons vu avec `StringIndexer`, un` Pipeline` est un estimateur. La méthode `pipeline.fit ()` renvoie un `PipelineModel`, qui est un transformateur.\n",
    "\n",
    "Pour plus d'informations:\n",
    "[Pipeline] (https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.Pipeline)\n",
    "[PipelineModel] (https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/PipelineModel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3c81d6e922b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define the pipeline based on the stages created in previous steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstringIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelToIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvecAssembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define the pipeline based on the stages created in previous steps.\n",
    "pipeline = Pipeline(stages=[stringIndexer, encoder, labelToIndex, vecAssembler, lr])\n",
    "\n",
    "# Define the pipeline model.\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "# Apply the pipeline model to the test dataset.\n",
    "predDF = pipelineModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the predictions from the model. The features column is a sparse vector, which is often the case after one-hot encoding, because there are so many 0 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8a66a031ae72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predDF' is not defined"
     ]
    }
   ],
   "source": [
    "display(predDF.select(\"features\", \"label\", \"prediction\", \"probability\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 4. Évaluer le modèle\n",
    "\n",
    "La commande `display` a une option de courbe ROC intégrée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we use the `BinaryClassificationEvaluator` to evalute the area under the ROC curve and the `MulticlassClassificationEvaluator` to evalute the accuracy.\n",
    "\n",
    "For more information:  \n",
    "[BinaryClassificationEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.BinaryClassificationEvaluator)  \n",
    "[MulticlassClassificationEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pipelineModel.stages[-1], predDF.drop(\"prediction\", \"rawPrediction\", \"probability\"), \"ROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "bcEvaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "print(f\"Area under ROC curve: {bcEvaluator.evaluate(predDF)}\")\n",
    "\n",
    "mcEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(f\"Accuracy: {mcEvaluator.evaluate(predDF)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 5. Réglage des hyperparamètres\n",
    "\n",
    "MLlib fournit des méthodes pour faciliter le réglage des hyperparamètres et la validation croisée.\n",
    "- Pour le réglage des hyperparamètres, `ParamGridBuilder` vous permet de définir une recherche de grille sur un ensemble d'hyperparamètres de modèle.\n",
    "- Pour la validation croisée, `CrossValidator` vous permet de spécifier un estimateur (le pipeline à appliquer à l'ensemble de données d'entrée), un évaluateur, un espace de grille d'hyperparamètres et le nombre de plis à utiliser pour la validation croisée.\n",
    "  \n",
    "Pour plus d'informations:\n",
    "[Sélection du modèle par validation croisée] (https://spark.apache.org/docs/latest/ml-tuning.html)\n",
    "[ParamGridBuilder] (https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.tuning)\n",
    "[CrossValidator] (https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez `ParamGridBuilder` et` CrossValidator` pour régler le modèle. Cet exemple utilise trois valeurs pour `regParam` et trois pour` elasticNetParam`, pour un total de 3 x 3 = 9 combinaisons d'hyperparamètres pour `CrossValidator` à examiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque fois que vous appelez `CrossValidator` dans MLlib, Databricks suit automatiquement toutes les exécutions en utilisant [MLflow] (https://mlflow.org/). Vous pouvez utiliser l'interface utilisateur de MLflow ([AWS] (https://docs.databricks.com/applications/mlflow/index.html) | [Azure] (https://docs.microsoft.com/azure/databricks/applications/ mlflow /)) pour comparer les performances de chaque modèle.\n",
    "\n",
    "Dans cet exemple, nous utilisons le pipeline que nous avons créé comme estimateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3-fold CrossValidator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=bcEvaluator, numFolds=3, parallelism = 4)\n",
    "\n",
    "# Run cross validations. This step takes a few minutes and returns the best model found from the cross validation.\n",
    "cvModel = cv.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 6. Faire des prédictions et évaluer les performances du modèle\n",
    "Utilisez le meilleur modèle identifié par la validation croisée pour faire des prédictions sur l'ensemble de données de test, puis évaluez les performances du modèle en utilisant la zone sous la courbe ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model identified by the cross-validation to make predictions on the test dataset\n",
    "cvPredDF = cvModel.transform(testDF)\n",
    "\n",
    "# Evaluate the model's performance based on area under the ROC curve and accuracy \n",
    "print(f\"Area under ROC curve: {bcEvaluator.evaluate(cvPredDF)}\")\n",
    "print(f\"Accuracy: {mcEvaluator.evaluate(cvPredDF)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
